#kafka common
kafka.brokers=
kafka.request.timeout.ms=
kafka.zookeeper.connect=
#kafka consumer
kafka.consumer.group.id=test_g1
kafka.consumer.auto.offset.reset=earliest
kafka.consumer.fetch.max.wait.ms=
kafka.consumer.fetch.max.bytes=
kafka.consumer.fetch.min.bytes=
kafka.consumer.isolation.level=
#kafka producer
kafka.producer.transaction.time.out=
kafka.producer.batch.size.config=
kafka.producer.max.block.ms=
kafka.producer.linger.ms=
kafka.producer.buffer.memory=
kafka.producer.compression.type=lz4
kafka.producer.topic=ua-event-sensor
kafka.producer.invalid.topic=ua-event-sensor-invalid-data
## rockdbs
stream.checkpoint.type=rocksdb
stream.checkpoint.interval=60000
stream.checkpoint.dir=file:///tmp/flink/tosa/checkpoint
stream.record.partition=saconsumerpartition
stream.record.offset=saconsumeroffset
#mysql.host=30.10.0.20
mysql.password=
#mysql.port=3983
mysql.username=
mysql.jdbc.url=
##metadata query
query.event.blacklist.sql=SELECT id,project_id as projectId,event_name as eventName,event_ch_name as eventChName,`status`,create_time as createTime,update_time as updateTime from spark_analyzer_event_blacklist where status=0
query.project.module.sql=select xe.project_id as projectId,xe.module_id as moduleId,xe.module_name as moduleName,xe.event_name as eventName,xe.event_alias_name as eventAliasName,p.page_name as pageName,1c.module_name as classify1Name,2c.module_name as classify2Name,3c.module_name as classify3Name,t.tag_name as tagName from ( select project_id,module_id,module as module_name,event_identifier as event_name,event_name as event_alias_name,page,classify1_id,classify2_id,classify3_id,event_tag from bury_point_event e where is_delete = '0') xe left join bury_point_module 1c on 1c.module_id = xe.classify1_id and 1c.module_level = '1' and 1c.is_delete = 0 left join bury_point_module 2c on 2c.module_id = xe.classify2_id and 2c.module_level = '2' and 2c.is_delete = 0 left join bury_point_module 3c on 3c.module_id = xe.classify3_id and 3c.module_level = '3' and 3c.is_delete = 0 left join bury_point_page p on p.page_id = xe.page and p.is_delete = 0 left join bury_point_tag t on t.tag_id = xe.event_tag
query.property.mapping.db.sql=select id ,project_id as projectId ,property_name as propertyName ,property_ch_name as propertyChName,property_name_sa as propertyNameSa,property_ch_name_sa as propertyChNameSa,`property_type` as propertyType,`status` as status ,create_time as createTime,update_time as updateTime from spark_analyzer_project_property
insert.checkpoint=insert into ua_project_kafka_offset (insert_time,topicname,partitionid,offset,group_name,jobid) values (?,?,?,?,?,?)
query.latest.checkpoint=select partitionid,max(offset) from ua_project_kafka_offset where topicname=? and group_name=? group by partitionid;
##metadata query interval
metadata.query.interval=60000
##for user relation delay push time
delay.push.time=10000
##限流
throttle.elementsPerSecond=1